# BIGR: HARNESSING BINARY LATENT CODES FOR IMAGE GENERATION AND IMPROVED VISUAL REPRESENTATION CAPABILITIES

> 这篇文章介绍了一个名为BiGR（Harnessing Binary Latent Codes for Image Generation and Improved Visual Representation Capabilities）的新型条件图像生成模型，它主要解决了以下问题：
>
> 1. **生成与表示能力的统一**：BiGR是第一个在同一框架内**统一生成和判别任务**的条件生成模型。它通过使用**紧凑的二进制潜在代码**进行生成训练，旨在增强生成和表示能力。
>
> 2. **提高图像生成质量**：通过使用FID-50k等指标衡量，BiGR在图像生成质量上展现出优越性能。
>
> 3. **提升表示能力**：通过线性探测准确率（linear-probe accuracy）来证明，BiGR在表示能力上也表现出色。
>
> 4. **零样本泛化能力**：BiGR展示了在多种视觉任务中的零样本泛化能力，包括图像修复（inpainting）、扩展（outpainting）、编辑、插值和丰富化（enrichment），无需结构修改。
>
> 5. **提高效率**：BiGR通过一种新颖的**按熵排序采样方法**，减少了采样迭代次数，显著加快了推理过程，提高了图像生成的效率。
>
> 6. **灵活性和可扩展性**：BiGR在不同的视觉应用中表现出灵活性，并且能够在生成和判别任务中展示出可扩展性。
>
> 文章通过广泛的实验验证了BiGR在图像生成质量、表示能力和零样本泛化应用方面的优越性能，并探讨了其在视觉任务中的应用潜力。

<img src="D:\learning\paper\论文笔记\AIGC\fig\BIGR.png" style="zoom:67%;" />

## 相关工作及缺陷

相关研究主要集中在图像生成和表示学习领域：

1. **基于重建的学习（Reconstruction-based Learning）**：
   - **缺陷**：[Balestriero & LeCun (2024)](https://arxiv.org/pdf/2402.11337) 指出，基于重建的学习虽然能够产生视觉上吸引人的结果，但**未能提供强大的潜在表示**用于感知任务。

2. **无条件生成模型的表示能力研究**：
   - **缺陷**：尽管无条件生成模型的表示能力得到了一定研究，但这些研究主要集中在无条件生成上，而对条件生成模型的表示能力研究相对有限。

3. **条件图像生成**：
   - **缺陷**：条件图像生成模型虽然能够产生视觉上吸引人的图像，但它们在下游判别任务中的表示能力较弱，因为条件引导在这些任务中缺失，减弱了特征与类别之间的关系。

4. **扩散模型（Diffusion Models）** 和 **自回归模型（Autoregressive Models）**：
   - **缺陷**：这些模型在图像生成方面表现出色，但它们的表示能力尚未得到充分研究。

5. **二进制潜在代码建模（Binary Latent Code Modeling）**：
   - **缺陷**：虽然二进制潜在代码因其紧凑性和离散性在视觉表示中显示出有效性，但最近对二进制tokenizers的研究才引起关注，且主要集中于查找自由量化和二进制自编码器。

6. **生成表示学习（Generative Representation Learning）**：
   - **缺陷**：虽然自监督方法在表示学习领域占主导地位，但这些方法涉及专门设计用于判别任务的判别设计，并不直接适用于条件图像生成。

7. **条件图像生成模型**：
   - **缺陷**：这些模型虽然能够产生视觉上吸引人的图像，但它们的表示能力很少被研究，BiGR旨在填补这一空白。

BiGR通过统一生成和判别任务，并使用紧凑的二进制潜在代码，旨在克服上述研究中的缺陷，提供一个在生成质量和表示能力上都表现出色的模型。BiGR的设计允许它在多种视觉任务中进行零样本泛化，这是之前模型所不具备的。通过这些改进，BiGR在图像生成和视觉表示领域提供了一个更全面和有效的解决方案。

## 方法

<img src="D:\learning\paper\论文笔记\AIGC\fig\BIGR_2.png" style="zoom:80%;" />

1. **二进制Tokenizer**：
   - 将像素级图像转换为**二进制潜在代码序列**。使用查找自由量化（lookup-free quantization）（[Yu et al., 2024](https://arxiv.org/pdf/2310.05737)， [Wang et al., 2023](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Binary_Latent_Diffusion_CVPR_2023_paper.pdf)）将图像编码成二进制格式，然后通过一个token索引来表示。

2. **掩码建模机制（Masked Modeling）**：
   - 采用掩码建模方法，这是一种修改自典型自回归的方法，它允许模型在不改变Llama模型结构的情况下扩展token间的交互。

3. **二进制转码器（Binary Transcoder）**：
   - 将连续特征转换为伯努利分布的二进制代码。使用伯努利扩散过程来生成二进制代码，并通过预测伯努利分布概率来实现。

4. **熵排序采样方法（Entropy-Ordered Sampling Method）**：
   - 一种新颖的采样方法，用于图像生成过程中迭代地揭开序列中的token，其顺序由预测的伯努利分布概率的熵大小决定。

5. **平均池化（Average Pooling）**：
   - 对BiGR的中间特征进行平均池化，以获得用于下游判别任务的视觉类别的全局表示。

6. **伯努利扩散过程（Bernoulli Diffusion Process）**：
   - 用于生成二进制代码，通过迭代去噪过程，从伯努利分布中采样生成最终的二进制代码。

7. **加权二进制交叉熵损失（Weighted Binary Cross-Entropy Loss）**：
   - 在训练过程中，使用加权二进制交叉熵损失来优化语言模型和去噪网络。

8. **迭代揭开token的生成过程**：
   - 在生成过程中，根据预测概率的熵大小来确定揭开token的顺序，从而加速推理过程。
