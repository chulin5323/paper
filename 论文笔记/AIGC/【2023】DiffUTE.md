# 【2023 2 NeurIPS】DiffUTE: Universal Text Editing Diffusion Model

[arxiv](https://arxiv.org/abs/2305.10825) [PDF](..\..\AIGC\Text-Image Editing\【2023】DiffUTE.pdf) [code](https://github.com/chenhaoxing/DiffUTE) 

> 现有生成模型在生成过程中，对于生成正确文本和文本风格的渲染有一定困难，本文提出了DiffUTE，旨在替换或者修改原图片中的文字，并不改变其原有的风格。模型中使用了笔划以及位置信息作为辅助，从而生成多语言字符，并设计了自监督学习框架以利用大量网络数据来提升模型的性能。

## 以往工作存在的缺陷

- 受限于生成英文文本
- 渲染或者修正文字结果不够真实

## 解决的问题

- 场景文字编辑的两大难点：
    - 如何在保持图像纹理的前提下转换文字风格
    - 如何保持编辑背景的一致性
- 使用笔划和位置信息作为辅助，使得模型能将不同风格的文字嵌入到图像背景中
- 设计了自监督学习框架，使模型从无标注数据中进行学习（对文字进行mask，随后在扩散过程中进行恢复）

## 方法

### 模型结构

![](D:\learning\论文\论文笔记\AIGC\DiffUTE_1.png)

- 模型输入：潜空间图像向量$z_t$，mask图像潜向量$x_m$，文本mask $m$三者的拼接
- 使用VAE将原始图像投射到潜空间，减少计算复杂度（**为了提升文本图像的重构性能，在文本数据集上进一步微调了扩散模型中使用的VAE**）
    - 使用递进式的训练策略：用于训练的图像尺寸随着训练阶段而不断增加
        - 在前三个阶段，随机裁剪图像为$S/8,S/4,S/2$，并重新resize为$S$用于训练，$S$是模型输入图像的分辨率并且$S=H=W$
        - 在第四个阶段，使用VAE输入图片相同大小的图片进行训练

### 细粒度条件控制

- 位置：使模型更关注文字生成的区域
- 笔划：将文字图片的潜在特征作为控制条件

### 自监督训练框架

- glyph encoder：使用了预训练OCR encoder(**Trocr**)作为字形编码器，字体风格统一
- 自监督训练流程：
    - 从图像中随机挑选一个ocr区域，该区域中相应的文字，通过统一风格的字体生成文字图片$x_g$
    - 重新生成的字符图像$x_g$作为glyph encoder的输入，并得到glyph embedding$e_g$
    - masked潜向量图像$x_m$，mask图像$m$，以及加噪的图像潜向量$z_t$进行拼接得到$z't$
    - $z'_t$经过卷积调整维度得到$\hat{z_t}$，并输入到UNet作为query向量，随后与条件进行cross attention

### 使用LLM进行交互场景文本编辑

- 使用ChatGLM理解用户输入的文本，从而不需要用户提供mask区域即可进行文本编辑